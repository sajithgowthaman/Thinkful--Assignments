{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wcC_gGA5bCCH"
   },
   "source": [
    "# Solutions to Checkpoint 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zF8evHJwbCCJ"
   },
   "source": [
    "## 1. Converting words or sentences into numeric vectors is fundamental when working with text data. To make sure you are solid on how these vectors work, please generate the tf-idf vectors for the last three sentences of the example we gave at the beginning of this checkpoint. If you are feeling uncertain, have your mentor walk you through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last three sentences:\n",
    "- 4 \"The Lumberjack Song is the funniest Monty Python bit: I can't think of it without laughing.\"\n",
    "- 5 \"I would rather put strawberries on my ice cream for dessert, they have the best taste.\"\n",
    "- 6 \"The taste of caramel is a fantastic accompaniment to tasty mint ice cream.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_4 = \"The Lumberjack Song is the funniest Monty Python bit: I can't think of it without laughing\"\n",
    "sentence_5 = \"I would rather put strawberries on my ice cream for dessert, they have the best taste.\"\n",
    "sentence_6 = \"The taste of caramel is a fantastic accompaniment to tasty mint ice cream.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWords4 = sentence_4.split(' ')\n",
    "bagOfWords5 = sentence_5.split(' ')\n",
    "bagOfWords6 = sentence_6.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove unique words by casting it to a set\n",
    "uniqueWords = set(bagOfWords4).union(set(bagOfWords5)).union(set(bagOfWords6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWords4 = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWords4:\n",
    "    numOfWords4[word] += 1\n",
    "numOfWords5 = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWords5:\n",
    "    numOfWords5[word] += 1\n",
    "numOfWords6 = dict.fromkeys(uniqueWords, 0)\n",
    "for word in bagOfWords6:\n",
    "    numOfWords6[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf4 = computeTF(numOfWords4, bagOfWords4)\n",
    "tf5 = computeTF(numOfWords5, bagOfWords5)\n",
    "tf6 = computeTF(numOfWords6, bagOfWords6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([numOfWords4, numOfWords5, numOfWords6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf4 = computeTFIDF(tf4, idfs)\n",
    "tfidf5 = computeTFIDF(tf5, idfs)\n",
    "tfidf6 = computeTFIDF(tf6, idfs)\n",
    "df = pd.DataFrame([tfidf4, tfidf5, tfidf6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>laughing</th>\n",
       "      <th>is</th>\n",
       "      <th>for</th>\n",
       "      <th>taste.</th>\n",
       "      <th>fantastic</th>\n",
       "      <th>Lumberjack</th>\n",
       "      <th>on</th>\n",
       "      <th>they</th>\n",
       "      <th>bit:</th>\n",
       "      <th>cream.</th>\n",
       "      <th>...</th>\n",
       "      <th>strawberries</th>\n",
       "      <th>funniest</th>\n",
       "      <th>accompaniment</th>\n",
       "      <th>it</th>\n",
       "      <th>my</th>\n",
       "      <th>tasty</th>\n",
       "      <th>of</th>\n",
       "      <th>best</th>\n",
       "      <th>taste</th>\n",
       "      <th>The</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025342</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>0.031190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.084509</td>\n",
       "      <td>0.031190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   laughing        is       for    taste.  fantastic  Lumberjack        on  \\\n",
       "0  0.068663  0.025342  0.000000  0.000000   0.000000    0.068663  0.000000   \n",
       "1  0.000000  0.000000  0.068663  0.068663   0.000000    0.000000  0.068663   \n",
       "2  0.000000  0.031190  0.000000  0.000000   0.084509    0.000000  0.000000   \n",
       "\n",
       "       they      bit:    cream.  ...  strawberries  funniest  accompaniment  \\\n",
       "0  0.000000  0.068663  0.000000  ...      0.000000  0.068663       0.000000   \n",
       "1  0.068663  0.000000  0.000000  ...      0.068663  0.000000       0.000000   \n",
       "2  0.000000  0.000000  0.084509  ...      0.000000  0.000000       0.084509   \n",
       "\n",
       "         it        my     tasty        of      best     taste       The  \n",
       "0  0.068663  0.000000  0.000000  0.025342  0.000000  0.000000  0.025342  \n",
       "1  0.000000  0.068663  0.000000  0.000000  0.068663  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.084509  0.031190  0.000000  0.084509  0.031190  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform([sentence_4, sentence_5, sentence_6])\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accompaniment</th>\n",
       "      <th>best</th>\n",
       "      <th>bit</th>\n",
       "      <th>can</th>\n",
       "      <th>caramel</th>\n",
       "      <th>cream</th>\n",
       "      <th>dessert</th>\n",
       "      <th>fantastic</th>\n",
       "      <th>for</th>\n",
       "      <th>funniest</th>\n",
       "      <th>...</th>\n",
       "      <th>song</th>\n",
       "      <th>strawberries</th>\n",
       "      <th>taste</th>\n",
       "      <th>tasty</th>\n",
       "      <th>the</th>\n",
       "      <th>they</th>\n",
       "      <th>think</th>\n",
       "      <th>to</th>\n",
       "      <th>without</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.320872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271642</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.210254</td>\n",
       "      <td>0.276458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276458</td>\n",
       "      <td>0.210254</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163281</td>\n",
       "      <td>0.276458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.328961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328961</td>\n",
       "      <td>0.250183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250183</td>\n",
       "      <td>0.328961</td>\n",
       "      <td>0.194290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.328961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accompaniment      best       bit       can   caramel     cream   dessert  \\\n",
       "0       0.000000  0.000000  0.271642  0.271642  0.000000  0.000000  0.000000   \n",
       "1       0.000000  0.276458  0.000000  0.000000  0.000000  0.210254  0.276458   \n",
       "2       0.328961  0.000000  0.000000  0.000000  0.328961  0.250183  0.000000   \n",
       "\n",
       "   fantastic       for  funniest  ...      song  strawberries     taste  \\\n",
       "0   0.000000  0.000000  0.271642  ...  0.271642      0.000000  0.000000   \n",
       "1   0.000000  0.276458  0.000000  ...  0.000000      0.276458  0.210254   \n",
       "2   0.328961  0.000000  0.000000  ...  0.000000      0.000000  0.250183   \n",
       "\n",
       "      tasty       the      they     think        to   without     would  \n",
       "0  0.000000  0.320872  0.000000  0.271642  0.000000  0.271642  0.000000  \n",
       "1  0.000000  0.163281  0.276458  0.000000  0.000000  0.000000  0.276458  \n",
       "2  0.328961  0.194290  0.000000  0.000000  0.328961  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 34 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eelbb1rRbCCK"
   },
   "source": [
    "* 4: 1.585, 1, 0, 1, 1.585, 0,0,0,0\n",
    "* 5: 0,0,0,0,0, .585, 1, 1.585, 1\n",
    "* 6: 0,0,0,0,0,0, 1, 0, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the cleaned novels. this can take a bit\n",
    "nlp = spacy.load('en')\n",
    "sentence4_doc = nlp(sentence_4)\n",
    "sentence5_doc = nlp(sentence_5)\n",
    "sentence6_doc = nlp(sentence_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n",
    "sentence4_doc = nlp(sentence_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for token in sentence4_doc:\n",
    "    if not token.is_punct:\n",
    "        sentences.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 9\n",
      "Tf_idf vector: [{}, {'lumberjack': 1.0}, {'song': 1.0}, {}, {}, {'funniest': 1.0}, {'monty': 1.0}, {'python': 1.0}, {'bit': 1.0}, {}, {'ca': 1.0}, {}, {'think': 1.0}, {}, {}, {}, {'laughing': 1.0}]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "sentences_tfidf=vectorizer.fit_transform(sentences)\n",
    "print(\"Number of features: %d\" % sentences_tfidf.get_shape()[1])\n",
    "\n",
    "# #splitting into training and test sets\n",
    "# X_train_tfidf, X_test_tfidf= train_test_split(sentences_tfidf, test_size=0.4, random_state=0)\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "sentences_tfidf_csr = sentences_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = sentences_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*sentences_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = sentences_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Tf_idf vector:', tfidf_bypara)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accompaniment', 'best', 'bit', 'can', 'caramel', 'cream', 'dessert', 'fantastic', 'for', 'funniest', 'have', 'ice', 'is', 'it', 'laughing', 'lumberjack', 'mint', 'monty', 'my', 'of', 'on', 'put', 'python', 'rather', 'song', 'strawberries', 'taste', 'tasty', 'the', 'they', 'think', 'to', 'without', 'would']\n",
      "(3, 34)\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    \"The Lumberjack Song is the funniest Monty Python bit: I can't think of it without laughing.\",\n",
    "     \"I would rather put strawberries on my ice cream for dessert, they have the best taste.\",\n",
    "     \"The taste of caramel is a fantastic accompaniment to tasty mint ice cream.\"\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAciQbgYbCCL"
   },
   "source": [
    "## 2. In the 2-grams example above, we only used 2-grams as our features. This time, use both 1-grams and 2-grams together as your feature set. Run the same models in the example and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFJkOg98bCCM",
    "outputId": "8e531f12-68a3-4243-9e2f-6b64870f1626"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/sajithgowthaman/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /Users/sajithgowthaman/opt/anaconda3/lib/python3.7/site-packages (2.1.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/sajithgowthaman/opt/anaconda3/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/sajithgowthaman/opt/anaconda3/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yj95ksrebCCU"
   },
   "outputs": [],
   "source": [
    "# utility function for standard text cleaning\n",
    "def text_cleaner(text):\n",
    "    # visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yJY4VGAbCCY"
   },
   "outputs": [],
   "source": [
    "# load and clean the data.\n",
    "persuasion = gutenberg.raw('austen-persuasion.txt')\n",
    "alice = gutenberg.raw('carroll-alice.txt')\n",
    "\n",
    "# the chapter indicator is idiosyncratic\n",
    "persuasion = re.sub(r'Chapter \\d+', '', persuasion)\n",
    "alice = re.sub(r'CHAPTER .*', '', alice)\n",
    "    \n",
    "alice = text_cleaner(alice)\n",
    "persuasion = text_cleaner(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLXe4lSGbCCa"
   },
   "outputs": [],
   "source": [
    "# parse the cleaned novels. this can take a bit\n",
    "nlp = spacy.load('en')\n",
    "alice_doc = nlp(alice)\n",
    "persuasion_doc = nlp(persuasion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tnbNkPa_bCCd",
    "outputId": "ff6bde31-0b69-4a79-c075-a62444ef8529"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Alice, was, beginning, to, get, very, tired, ...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(So, she, was, considering, in, her, own, mind...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(There, was, nothing, so, VERY, remarkable, in...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Oh, dear, !)</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   author\n",
       "0  (Alice, was, beginning, to, get, very, tired, ...  Carroll\n",
       "1  (So, she, was, considering, in, her, own, mind...  Carroll\n",
       "2  (There, was, nothing, so, VERY, remarkable, in...  Carroll\n",
       "3                                      (Oh, dear, !)  Carroll\n",
       "4                                      (Oh, dear, !)  Carroll"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group into sentences\n",
    "alice_sents = [[sent, \"Carroll\"] for sent in alice_doc.sents]\n",
    "persuasion_sents = [[sent, \"Austen\"] for sent in persuasion_doc.sents]\n",
    "\n",
    "# combine the sentences from the two novels into one data frame\n",
    "sentences = pd.DataFrame(alice_sents + persuasion_sents, columns = [\"text\", \"author\"])\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_AAoxFL3bCCg"
   },
   "outputs": [],
   "source": [
    "# get rid off stop words and punctuation\n",
    "# and lemmatize the tokens\n",
    "for i, sentence in enumerate(sentences[\"text\"]):\n",
    "    sentences.loc[i, \"text\"] = \" \".join(\n",
    "        [token.lemma_ for token in sentence if not token.is_punct and not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGjbNnmjbCCh",
    "outputId": "7144de1c-ec51-4137-ec2b-71493bdf5d1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abide</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>able bear</th>\n",
       "      <th>able persuade</th>\n",
       "      <th>abominate</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absence home</th>\n",
       "      <th>absent</th>\n",
       "      <th>...</th>\n",
       "      <th>young people</th>\n",
       "      <th>young person</th>\n",
       "      <th>young sister</th>\n",
       "      <th>young woman</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth say</th>\n",
       "      <th>zeal</th>\n",
       "      <th>zealous</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Alice begin tired sit sister bank have twice p...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>consider mind hot day feel sleepy stupid pleas...</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>remarkable Alice think way hear Rabbit</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>oh dear</td>\n",
       "      <td>Carroll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5521 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abide  ability  able  able bear  able persuade  abominate  abroad  absence  \\\n",
       "0    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "1    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "2    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "3    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "4    0.0      0.0   0.0        0.0            0.0        0.0     0.0      0.0   \n",
       "\n",
       "   absence home  absent  ...  young people  young person  young sister  \\\n",
       "0           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "1           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "2           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "3           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "4           0.0     0.0  ...           0.0           0.0           0.0   \n",
       "\n",
       "   young woman  youth  youth say  zeal  zealous  \\\n",
       "0          0.0    0.0        0.0   0.0      0.0   \n",
       "1          0.0    0.0        0.0   0.0      0.0   \n",
       "2          0.0    0.0        0.0   0.0      0.0   \n",
       "3          0.0    0.0        0.0   0.0      0.0   \n",
       "4          0.0    0.0        0.0   0.0      0.0   \n",
       "\n",
       "                                                text   author  \n",
       "0  Alice begin tired sit sister bank have twice p...  Carroll  \n",
       "1  consider mind hot day feel sleepy stupid pleas...  Carroll  \n",
       "2             remarkable Alice think way hear Rabbit  Carroll  \n",
       "3                                            oh dear  Carroll  \n",
       "4                                            oh dear  Carroll  \n",
       "\n",
       "[5 rows x 5521 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, min_df=2, use_idf=True, norm=u'l2', smooth_idf=True, ngram_range=(1,2))\n",
    "\n",
    "\n",
    "# applying the vectorizer\n",
    "X = vectorizer.fit_transform(sentences[\"text\"])\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "sentences = pd.concat([tfidf_df, sentences[[\"text\", \"author\"]]], axis=1)\n",
    "\n",
    "# keep in mind that the log base 2 of 1 is 0,\n",
    "# so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4mSCxp5bCCj",
    "outputId": "ded92b2d-d204-4f07-9ffc-6ddeb8d522c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Logistic Regression Scores----------------------\n",
      "Training set score: 0.9014162732574285\n",
      "\n",
      "Test set score: 0.870887130362349\n",
      "----------------------Random Forest Scores----------------------\n",
      "Training set score: 0.964454318244932\n",
      "\n",
      "Test set score: 0.870887130362349\n",
      "----------------------Gradient Boosting Scores----------------------\n",
      "Training set score: 0.8625381838378229\n",
      "\n",
      "Test set score: 0.8513119533527697\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = sentences['author']\n",
    "X = np.array(sentences.drop(['text','author'], 1))\n",
    "\n",
    "# We split the dataset into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.4, random_state=123)\n",
    "\n",
    "# Models\n",
    "lr = LogisticRegression()\n",
    "rfc = RandomForestClassifier()\n",
    "gbc = GradientBoostingClassifier()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rfc.fit(X_train, y_train)\n",
    "gbc.fit(X_train, y_train)\n",
    "\n",
    "print(\"----------------------Logistic Regression Scores----------------------\")\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Random Forest Scores----------------------\")\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))\n",
    "\n",
    "print(\"----------------------Gradient Boosting Scores----------------------\")\n",
    "print('Training set score:', gbc.score(X_train, y_train))\n",
    "print('\\nTest set score:', gbc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4809ELZCbCCm"
   },
   "source": [
    "As can be seen above, using 1-gram along with 2-gram improved the performances of all of the models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "5.solution_feature_engineering_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
